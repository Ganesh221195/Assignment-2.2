1.HDFS

     Hadoop File System was developed using distributed file system design 
     It is run on commodity hardware.
     HDFS is highly faulttolerant and designed using low-cost hardware
     HDFS holds very large amount of data and provides easier access
     To store such huge data, the files are stored across multiple machines  
     It is suitable for the distributed storage and processing
     It has two types of node
     
      *name node
      *data node

      name node - The namenode is the commodity hardware that contains name node server
                It is a software that can be run on commodity hardware
		The system having the namenode acts as the master server 
		It also executes file system operations such as renaming, closing, and opening files and directories
      
      data node - The datanode is a commodity hardware having the data node server
		For every node in a cluster, there will be a datanode. These nodes manage the data storage of their system
		Datanodes perform read-write operations on the file systems, as per client request

===================================================================================================================================================


2.Hadoop cluster
               
               *A Hadoop cluster is a special type of computational cluster designed specifically for storing and analyzing huge amounts of unstructured data in a distributed computing environment
	       Such clusters run Hadoop's open source distributed processing software on low-cost commodity computers
               *one machine in the cluster is designated as the NameNode and another machine the as JobTracker these are the masters 
               *The rest of the machines in the cluster act as both DataNode and TaskTracker these are the slaves 
               *Hadoop clusters are often referred to as shared nothing systems because the only thing that is shared between nodes is the network that connects them
               *Hadoop clusters are known for boosting the speed of data analysis applications
               *They also are highly scalable 
               *If a cluster's processing power is overwhelmed by growing volumes of data, additional cluster nodes can be added to increase throughput 
               *Hadoop clusters also are highly resistant to failure because each piece of data is copied onto other cluster nodes which ensures that the data is not lost if one node fails
	       *Large Hadoop Clusters are arranged in several racks
	      


========================================================================================================================================================================================================


3.HDFS blocks 
             
             *Hadoop distributed file system also stores the data in terms of blocks
             *The default size of HDFS block is 128MB
             *The files are split into 128MB blocks and then stored into the hadoop filesystem 
             *The hadoop application is responsible for distributing the data blocks across multiple nodes 
	     *The blocks are of fixed size so it is very easy to calculate the number of blocks that can be stored on a disk
             *HDFS block concept simplifies the storage of the datanodes
             *The datanodes doesn’t need to concern about the blocks metadata data like file permissions 
             *The namenode maintains the metadata of all the blocks
             *Blocks are easy to replicate between the datanodes and thus provide fault tolerance and high availability
 	     *Hadoop framework replicates each block across multiple nodes 
             *The default replication factor is 3
             


